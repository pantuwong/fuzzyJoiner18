\section{Related Work}
Extensions in data management systems for joins typically use string similarity algorithms such as edit-distance, Jaro-Winkler and TF-IDF; e.g., \cite{Cohen2003}.  String matching algorithms work poorly for merging the same entity because valid transformations of entity names can yield very different strings.  More recently, data driven approaches mine patterns to determine the `rules' for joining a given entity type.  One example is \cite{He:2015:SJS:2824032.2824036}, which determines which cell values should be joined based on whether those cell values co-occur on the same row across disparate tables in a very large corpus of data.  Another example is \cite{auto-join-joining-tables-leveraging-transformations} where program synthesis techniques are used to learn the right set of transformations needed to perform the entity matching operation.  Our approach for merging datasets is more general because the mapping function generalizes the set of transformations that are allowed across surface forms of an entity, even if they cannot be directly expressed as program transforms.

Joint embeddings have been applied recently to linking relational tuples for entity resolution \cite{Mudgal},\cite{Bordawekar18}.  Here, each model is specific to the database on which it was trained because the attributes of an entity vary among databases.  Our focus is on techniques for name matching, and we develop general purpose embedding models for merging alternate surface forms of key entities.  

Metric learning is a well studied problem in face recognition, e.g., \cite{DBLP:conf/cvpr/SchroffKP15}, with a rich literature in triplet mining techniques.  The closest approach to ours is the use of nearest neighbors algorithms for semi-hard triplet mining \cite{DBLP:journals/corr/KumarHC0D17}.  For semi-hard triplet mining, one cannot look at fixed neighborhood sizes in building triplets.  If all the positives are further away from the anchor than the negatives in a given neighborhood size of $k$, it means that $k$ needs to be expanded until a neighborhood size is found that has the right characteristics.  Our approach in including `hard negatives' means we can use a fixed $k$ to generate samples.  An additional benefit is that at least for certain types of datasets, we show that metric learning with hard negatives is more effective than semi-hard mining.

The study of loss function effectiveness in metric learning is also a rich literature, with two basic types of loss functions that have been proposed: (a) local loss functions such as triplet loss \cite{DBLP:conf/cvpr/SchroffKP15}, angular loss \cite{Zhang:2016:DML:3088616.3088665} and improved loss \cite{DBLP:journals/corr/abs-1708-01682}, and (b) loss functions that operate on a more global level across a batch of training examples \cite{NIPS2016_6200}, \cite{DBLP:conf/cvpr/SongXJS16}, \cite{songCVPR17}.   Since our triplet selection is global, rather than batch based, we did not see the value of using global loss functions.
