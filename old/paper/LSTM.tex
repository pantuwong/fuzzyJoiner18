\subsection{LSTM}
A regular Deep Neural network would treat the name like a bag of words, without accounting for order. We experimented with using an LSTM architecture to account for order the words apear in. This makes sense for our use case since first name and last names are different from eachother. In the implimentation we used GRUs (Gated Recurrent Unit), which function like LSTMs but are computationally cheaper.