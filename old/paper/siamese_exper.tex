\subsection{Siamese Network}
Our first experiment was using a Siamese network to match already blocked entities. The architecture of the network was as shown in Figure \ref{siamese}. As described above this involves two networks that share weights mapping the inputs onto a vector space and computing distance. Before feeding the entities into the networks, we used Kazuma character embeddings to encode each entity as a vector. We used character level embeddings since many names would not have been represented at all if we had used word level embeddings. We then trained the network on our 182772 pairs of names pulled from DBpedia. These are the pairs we have after running the data through the cleanser. We used 95\% of them for training and withheld 5\% for testing. In addition to these pairs we created an equal number of negative pairs to train the model. Wherever possible, the negative pairs had at least one word in common with each other. This was done so that the model would not just learn the obvious function of reject unrelated pairs. We trained the model in 10 epochs. We found that the fscore on the training data was .89 on the test data. This showed that this architecture is an effective matcher. However we only successfully included the correct match in around 15\% of pairs in the top five. This was barely better than the 5\% we got from the embedding alone.
\subsection{Triplet Loss}
The triplet loss experiment was extremely similar to the Siamese one, only we used the Triplet loss network instead of the Siamese one. We used a margin of 1. We also used a GRU instead of a regular deep neural network to capture the order of the words. The last change we used was in selection of negative points, instead of picking them based on some rule, we picked the closest 40 points that were not matches from the character embedding. We did this since those names are the hardest ones for the model to learn, this also avoids the problem in Figure \ref{comp_dif_fail}. When we did this we got 69\% of the items in the top 40. This is a relatively effective blocker. It also correctly placed 97\% of positives closer to the anchor than negatives, making it a very effective matcher.