\section{Introduction}

Joining two datasets is a key step in preparing the data for subsequent operations such as performing business analytics, building predictive models etc.  Data management systems have largely focussed on solely equi-joins, which is based on exact equality.  However, in the reality, string equality is inadequate because the same entity name in many cases might be expressed in slight variants of the same name, as shown in Table ~\ref{table-example}, where the same entity appears as slightly different variants (\textit{Douglas Adams}, \textit{Douglas Noel Adams}).  Performing this sort of name matching type join is currently a manual, error prone process.

One of the common ways of performing name matching is using variants of string similarity measures such as edit-distance, Jaro-Winkler, TF-IDF (e.g., \cite{Cohen2003}), with a strategy to reduce the number of pairs to consider based on prefix, length, and suffix based filtering of strings.  More recently, \textit{data driven} approaches to the problem of joins have emerged as an alternative to string matching techniques.  Data driven approaches are very powerful because they can perform 'semantic joins' such as mapping country names to country codes based on a large corpus of data to determine cell values that should be joined (e.g., \cite{He:2015:SJS:2824032.2824036}).  That is, they rely on a dictionary style lookup that is built from correlations in data rather than strin similarity.  Another example of a data driven approach is work by \cite{auto-join-joining-tables-leveraging-transformations} that uses program synthesis techniques to learn the right set of transformations needed to perform the name matching operation, based on a numerous examples.

In this paper, we propose a novel data driven approach to the problem of joining semantically different representations of data.  Our approach relies on building supervised deep learning models to automatically learn the correct set of transformations needed to compute the equality of two cell values.  Because a deep learning model can in theory pick up the appropriate features of what should be used for matching from correlations in the data, it should be able to generalize better across join problems than the data driven approaches outlined in \cite{He:2015:SJS:2824032.2824036} or \cite{auto-join-joining-tables-leveraging-transformations}.  However, deep learning models need a significantly larger set of examples than what might be needed by an apprpach such as the one outlined in \cite{auto-join-joining-tables-leveraging-transformations}.  Nevertheless, we strongly believe that as the need for automated data preparation techniques grows, the problem of exploring data driven approaches to perform difficult data preparation steps are becoming increasingly important.

Our specific solution to the join problem involves building a deep neural network that learns to produce a small distance estimate for elements of a name pair that represent the same entity (e.g., \textit{Douglas Adams}-\textit{Douglas Noel Adams} should produce a distance estimate \textit{d} that is closest to 0), and a much larger distance estimate for elements of the name pair that do not represent the same entity (e.g., \textit{Douglas Adams}-\textit{John Adams} should produce a distance estimate \textit{d} that is greater than some margin \textit{m}).  The function learnt by such a network (often called a \textit{siamese network}) is conceptually one that maps input vectors for the same entity closer together in vector space, while mapping input vectors for a different entity to a distance that is at least \textit{m} distance away from the vectors for the same entity, as shown in Figure \ref{fig-1}.  This sort of function can be used to determine join equality on a subset of string pairs that are considered \textit{joinable} using some sort of filtering algorithm, as is often done in string similarity based joins.  As an example, \textit{q-grams} are often used as a mechanism to filter out the subset of strings that should be compared (\cite{auto-join-joining-tables-leveraging-transformations}.  However, our observation is that one can actually exploit what the siamese network produces to eliminate this filtering step altogether.  Specifically, the last hidden layer of the siamese network effectively is the \textit{vector embedding} for the same versus different estimate.  That is, the last layer is a vector in a lower dimensional space that contains the critical features needed for computing the distance estimate.  We can in fact take these \textit{vector embeddings} for all vectors in the two tables to be joined, and use approximate nearest neighbors algorithms to find the nearest neighbors.  Ideally, the nearest neighbor that has a distance lower the margin \m should be the correct match.  Because approximate nearest neighbor algorithms have been applied successfully to millions of items, this approach should scale just as well as filtering/join algorithms, but have the added advantage that the filtering step is also data driven, and hence can adapt to different entity types very effectively.

Our key contributions to the join problem are as follows:
\begin{itemize}
\item We demonstrate that we can use siamese networks to learn a distance function that successfully discriminates same pairs from distance pairs with an F score of .9.  Our results suggest that deep learning models can be used successfully as another mechanism for data driven joins, assuming some sort of filtering approach has been applied to the data.
\item We explore whether it is possible to extend this further, which is to see if the output of the function can in fact be used in an approximate nearest neighbor algorithm to identify neighbors to match with, without any filtering.  
\end{itemize}

