\section{Introduction}

Merging two datasets using joins is a key operation in data management.  Data management systems have largely focussed solely on equi-joins, where equality of strings or numeric values of the two columns determines if rows should be joined.  However, equality at a string level is inadequate because the same entity may in many cases be expressed with slight variations of the same name, as shown in Table~\ref{table-example} (e.g., \textit{Douglas Adams} and \textit{Douglas Noel Adams}).  This problem occurs for many entity types such as company names, addresses, product descriptions, and conference venues.

A common technique to overcome this problem is to perform fuzzy joins using string similarity algorithms such as edit-distance, Jaro-Winkler and TF-IDF (e.g., \cite{Cohen2003}).  In applying fuzzy joins, a key problem is to avoid quadratic comparisons between pairs of strings drawn from the two columns.  A blocking strategy is used to reduce the number of pairs for fuzzy joins, such that only strings that share a common prefix, or suffix will be compared further.  String matching algorithms often do not work because transformations of entity names in perfectly valid ways can yield very different strings (e.g., switching the last name and first name of a person, or providing acronyms for a company name).

More recently, data driven approaches have emerged as a powerful alternative to string matching techniques for fuzzy joins.  Data driven approaches mine patterns in the data to determine the `rules' for joining a given entity type.  One example of such an approach is illustrated in \cite{He:2015:SJS:2824032.2824036}, which determines which cell values should be joined based on whether those cell values co-occur on the same row across disparate tables in a very large corpus of data.  Another example of a data driven approach is work by \cite{auto-join-joining-tables-leveraging-transformations} where program synthesis techniques are used to learn the right set of transformations needed to perform the entity matching operation.  

In this paper, we propose a novel data driven approach to the problem of joining different representations of the same entity.  Our approach relies on building supervised deep learning models to automatically learn the correct set of transformations needed to compute the equality of two cell values.  For the closely related problem of face recognition and person re-identification, deep learning networks have been used successfully to discriminate pairs that represent the same entity from those that don't.  However, we know of no work that has adapted these techniques to the problem of fuzzy joins for entity names.

Because a deep learning model can learn the features necessary for matching from the data, it should in theory be able to generalize better to new datasets than the data driven approaches outlined in \cite{He:2015:SJS:2824032.2824036}.   Conceptually, our approach is similar in spirit to the approach described in \cite{auto-join-joining-tables-leveraging-transformations}, but we use deep neural networks to learn the right function for the join operation instead of synthesizing different programs for different entity types.  The advantage of building such a function over the approach described in \cite{auto-join-joining-tables-leveraging-transformations} is that we can use this function in novel ways to completely eliminate the blocking step that is typically needed to identify the set of potentially joinable rows, as we describe below.

Our solution to the join problem involves building a system that performs deep metric learning.  That is, the specific function the network learns is to map input vectors corresponding to same entity closer in vector space compared to vectors that correspond to different entities.  As in face recognition, we do this by training a `triplet siamese network'.  However, entity names have very different properties from faces, so we needed to adapt the approaches used in face recognition to the entity matching problem with a novel method to choose the sample for training, and novel loss functions as objectives for the metric learning problem.  

At training, the network is given a triplet consisting of an `anchor' (e.g. \textit{Douglas Adams}), a positive element (e.g. \textit{Douglas Noel Adams}) and a negative element (e.g., John Adams), and it learns to produce a small distance estimate for the positive pair (close to 0), and a large distance estimate for the negative pair, using an objective function. A classification form of this function that simply learns whether two names belong to the same entity can be used to perform fuzzy joins, but then we would need a blocking strategy analogous to what has been used in the literature to identify the set of joinable pairs \cite{auto-join-joining-tables-leveraging-transformations}.  

Our observation is that one can actually exploit what the siamese network produces to eliminate this blocking step altogether.  Specifically, the last hidden layer of the siamese network is effectively a `vector embedding' that contains the critical features needed for computing the distance estimate.  If such models are pre-built for a given entity type, we can then use the models to produce vector embeddings for all cell values in the two columns to be joined in a single linear pass, and index just the second column values in an approximate nearest neighbors index.  Then, for each cell value in the first column to be joined, querying the k-nearest neighbors should provide the set of values in the second column that can be joined.  Because approximate nearest neighbor algorithms have been applied successfully to billions of items \cite{JDH17}, this approach should scale just as well as join algorithms based on blocking, but has the added advantage that the blocking step is also data driven, and hence can adapt to different entity types very effectively.

\begin{table}[!htb]
    \caption{Example of a join problem}
    \begin{subtable}{.5\linewidth}
      \centering
        \caption{Table A}
        \begin{tabular}{|l|l|}
          \hline
           Dept & Name \\
           \hline
           1    & Douglas Adams \\
           2    & John Adams \\
           3  & Douglas Bard \\
           \hline
        \end{tabular}
    \end{subtable}%
    \begin{subtable}{.5\linewidth}
      \centering
        \caption{Table B}
        \begin{tabular}{|l|l|}
          \hline
           Name & Salary \\
           \hline
           Douglas N. Adams & 2000 \\
           Adams, John & 3000 \\
           John Smith & 3000 \\
           \hline
        \end{tabular}
    \end{subtable}
    \label{table-example}
\end{table}

Our key contributions are as follows:
\begin{itemize}
\item We demonstrate that triplet loss based siamese networks can be used to learn a distance function that successfully discriminates same entity pairs from different pairs with an accuracy of 98\% and 97\% for about 200,000 people's names and 70,000 company names respectively, drawn from Wikidata.  More importantly, we show that such a network has a high recall of 83\% and 79\%, finding the variants of an entity name in the top 20 neighbors for people and companies respectively.  Moreover, within the 20 neighbors, 77\% and 75\% of the positive names were closer to the anchor than negative names for people and companies respectively.  Our results suggest that deep learning models can be used successfully for data driven joins, assuming that such models are built for common entity types.
\item Training neural networks for deep metric learning is difficult because one cannot show quadratic combinations of the data to the system.  In face recognition, a number of techniques exist for triplet selection, and methods for selecting triples is an active area of research.  We developed a new technique for triplet selection for entity names because the techniques from face recognition do not generalize well to entity names.
\item We also developed a novel loss function to train the network to discriminate entity names from names of other entities.  We compare the results for this loss function with the ones that have been used for face recognition, and show that this loss function can perform much better than the existing functions for deep learning in this domain.
\end{itemize} 

The rest of the paper is organized as follows: Section~\ref{siamese networks} describes how we adapted siamese networks to the problem of entity matching, specifically the problem of triplet selection for entity names, and the need for a new type of loss function to improve metric learning for this problem, section ~\ref{dataset} describes characteristics of the datasets we used for training, as well as cleansing and data augmentation techniques we used to generate data, section ~\ref{results} provides the experimental results, section~\ref{related work} covers related work, and section~\ref{conclusions} describes future work and conclusions.
