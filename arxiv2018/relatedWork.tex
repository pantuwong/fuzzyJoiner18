\section{Related Work}
Extensions in data management systems for handling joins typically use string similarity algorithms such as edit-distance, Jaro-Winkler and TF-IDF; e.g., \cite{Cohen2003}.  String matching algorithms often do not work for merging different forms of the same entity because valid transformations of entity names can yield very different strings.  More recently, data driven approaches have emerged as a powerful alternative for merging data.  Data driven approaches mine patterns to determine the `rules' for joining a given entity type.  One example of such an approach is illustrated in \cite{He:2015:SJS:2824032.2824036}, which determines which cell values should be joined based on whether those cell values co-occur on the same row across disparate tables in a very large corpus of data.  Another example is work by \cite{auto-join-joining-tables-leveraging-transformations} where program synthesis techniques are used to learn the right set of transformations needed to perform the entity matching operation.  Our approach for merging datasets is much more general than either approach because the mapping function generalizes the set of transformations that are allowed across surface forms of an entity, even if they cannot be directly expressed as program transforms.

The idea of building joint embeddings for merging datasets followed by nearest neighbors search has been applied recently to the problem of linking relational tuple embeddings with embeddings of other relational tuples or unstructured text \cite{Bordawekar18}, \cite{IDEL18}).  For the problem of linking tuples, each model that is learnt is specific to the database it was trained on.  Our focus is on techniques that can be used to develop general purpose embedding models for merging alternate surface forms of key entities.  Once such models are built, they can be applied to joining any two datasets that share that semantic type.

Metric learning is a well studied problem in the face recognition literature, e.g., \cite{DBLP:conf/cvpr/SchroffKP15}, with a rich literature in triplet mining techniques.  The closest approach to ours is the use of nearest neighbors algorithms for semi-hard triplet mining \cite{DBLP:journals/corr/KumarHC0D17}.  For semi-hard triplet mining, one cannot look at fixed neighborhood sizes in building triplets.  If all the positives are further away from the anchor than the negatives in a given neighborhood size of $k$, it means that $k$ needs to be expanded until a neighborhood size is found that has the right characteristics.  Our approach in including `hard negatives' means we can use a fixed $k$ to generate samples.  An additional benefit is that at least for certain types of datasets, we show that metric learning with hard negatives is more effective than semi-hard mining.

The study of loss function effectiveness in metric learning is also a rich literature, with two basic types of loss functions that have been proposed: (a) local loss functions such as triplet loss \cite{DBLP:conf/cvpr/SchroffKP15}, angular loss \cite{Zhang:2016:DML:3088616.3088665} and improved loss \cite{DBLP:journals/corr/abs-1708-01682}, and (b) loss functions that operate on a more global level across a batch of training examples \cite{NIPS2016_6200}, \cite{DBLP:conf/cvpr/SongXJS16}, \cite{songCVPR17}.   Since our triplet selection is global, rather than batch based, we did not see the value of using global loss functions.
