\section{Introduction}

Merging datasets is a key operation for data analytics.  A frequent
requirement for merging is joining across columns that have
different surface forms for the same entity (e.g., the name of a
person might be represented as \textit{Douglas Adams},
\textit{Adams, Douglas} or \textit{Douglas Noel Adams}).  Similarly,
ontology alignment can require recognizing distinct surface forms of
the same entity, especially when ontologies are independently
developed.  This problem occurs for many entity types such as people's names, company names, addresses, product descriptions, conference venues, or even people's faces.  Data management systems have however, largely focussed solely on equi-joins, where string or numeric equality determines which rows should be joined, because such joins are efficient.

Extensions in data management systems for handling `fuzzy' joins across different surface forms for the same entity typically use string similarity algorithms such as edit-distance, Jaro-Winkler and TF-IDF (e.g., \cite{Cohen2003}).  In applying fuzzy joins, a key problem is to avoid quadratic comparisons between pairs of strings drawn from the two columns.  A blocking strategy is used to reduce the number of pairs for fuzzy joins, e.g. only strings that share a common prefix or suffix will be compared further.  But such strategies based on strings often do not work because transformations of entity names in perfectly valid ways can yield very different strings.  As shown in Table~\ref{table-example}, \textit{John Smith} is more similar to \textit{John Adams} than \textit{Adams, John}, but of course it is \textit{Adams, John} that is the valid alternate surface form for \textit{John Adams}.

More recently, data driven approaches have emerged as a powerful alternative to string matching techniques for fuzzy joins.  Data driven approaches mine patterns in the data to determine the `rules' for joining a given entity type.  One example of such an approach is illustrated in \cite{He:2015:SJS:2824032.2824036}, which determines which cell values should be joined based on whether those cell values co-occur on the same row across disparate tables in a very large corpus of data.  Another example of a data driven approach is work by \cite{auto-join-joining-tables-leveraging-transformations} where program synthesis techniques are used to learn the right set of transformations needed to perform the entity matching operation.  

In this paper, we propose a novel approach to the problem of joining different surface representations of the same entity, inspired in part by recent advances in deep learning.  Our approach relies on building supervised deep learning models to automatically learn the correct set of transformations needed to compute the equality of two cell values.  Because a deep learning model can learn the features necessary for matching from the data, it should in theory be able to generalize better to new datasets than the data driven approaches outlined in \cite{He:2015:SJS:2824032.2824036}.   Conceptually, our approach is similar in spirit to the approach described in \cite{auto-join-joining-tables-leveraging-transformations}, but we use deep neural networks to learn the right function for the join operation instead of synthesizing different programs for different entity types.  The advantage of building such a function compared to work described in \cite{auto-join-joining-tables-leveraging-transformations} is that (a) we can use this function in novel ways to completely eliminate the blocking step that is typically needed to identify the set of potentially joinable rows, as we describe below, and (b) we can use this function on arbitrary surface forms such as faces, where valid transformations of a face cannot be readily described in terms of a program.

Specifically, our solution to the join problem involves building a system that performs deep metric learning: the network learns to map input vectors corresponding to same entity closer in vector space than vectors corresponding to different entities.  For the closely related problem of face recognition and person re-identification, deep learning networks have been used successfully to discriminate pairs that represent the same entity from pairs that do not.  However, simply building a model that discriminates pairs of entities is not sufficient; if used naively, this would mean a quadratic comparison between cell values in the two columns to be joined.  We know of no prior work that has adapted these techniques to the problem of how to use such deep learning models to perform joins efficiently for different surface forms of the same entity.  

As in face recognition, our system for metric learning is built by training a `triplet siamese network' to learn to map different surface forms of the same entity closer in vector space than different forms of the same entity.  Entity names have very different properties from faces, so we needed to adapt the approaches used in face recognition to the problem of matching entity names.  We use a novel method to choose the sample for training, and novel loss functions as objectives for the metric learning problem.  At training, the network is given a triplet consisting of an `anchor' (e.g. \textit{Douglas Adams}), a positive element (e.g. \textit{Adams, Douglas}) and a negative element (e.g., \textit{John Adams}), and it learns to produce a small distance estimate for the positive pair (close to 0), and a large distance estimate for the negative pair, using an objective function. A classification form of this function that simply learns whether two names belong to the same entity can be used to perform fuzzy joins, but then we would need a blocking strategy analogous to what has been used in the literature to identify the set of joinable pairs \cite{auto-join-joining-tables-leveraging-transformations}.  

Our observation is that one can actually exploit what the siamese network produces to eliminate this blocking step altogether.  Specifically, the last hidden layer of the siamese network is effectively a `vector embedding' that contains the critical features needed for computing the distance estimate.  If such models are pre-built for a given entity type, we can then use the models to produce vector embeddings for all cell values in the two columns to be joined in a single linear pass, and index just the second column values in an approximate nearest neighbors index.  Then, for each cell value in the first column to be joined, querying the k-nearest neighbors should provide the set of values in the second column that can be joined.  Because approximate nearest neighbor algorithms have been applied successfully to billions of items \cite{JDH17}, this approach should scale just as well as join algorithms based on blocking, but it has the added advantage that the blocking step is also data driven, and hence can adapt to different entity types very effectively.

\begin{table}[!htb]
    \caption{Example of a merge problem}
    \begin{subtable}{.5\linewidth}
      \centering
        \caption{Table A}
        \begin{tabular}{|l|l|}
          \hline
           Dept & Name \\
           \hline
           1    & Douglas Adams \\
           2    & John Adams \\
           3  & Douglas Bard \\
           \hline
        \end{tabular}
    \end{subtable}%
    \begin{subtable}{.5\linewidth}
      \centering
        \caption{Table B}
        \begin{tabular}{|l|l|}
          \hline
           Name & Salary \\
           \hline
           Douglas Noel Adams & 2000 \\
           Adams, John & 3000 \\
           John Smith & 3000 \\
           \hline
        \end{tabular}
    \end{subtable}
    \label{table-example}
\end{table}

Our key contributions are as follows:
\begin{itemize}
\item We demonstrate that triplet loss based siamese networks can be used to learn a distance function that successfully discriminates same entity pairs from different pairs with an accuracy of 98\% and 97\% for about 200,000 people's names and 70,000 company names respectively, drawn from Wikidata.  More importantly, we show that such a network has a high recall of 83\% and 79\%, finding the variants of an entity name in the top 20 neighbors for people and companies respectively.  Moreover, within the 20 neighbors, 77\% and 75\% of the positive names were closer to the anchor than negative names for people and companies respectively.  Our results suggest that deep learning models can be used successfully for data driven joins, assuming that such models are built for common entity types.
\item Training neural networks for deep metric learning is difficult because one cannot show quadratic combinations of the data to the system.  In face recognition, a number of techniques exist for triplet selection, and methods for selecting triples is an active area of research.  We developed a new technique for triplet selection for entity names because the techniques from face recognition do not generalize well to entity names.
\item We also developed a novel loss function to train the network to discriminate entity names from names of other entities.  We compare the results for this loss function with the ones that have been used for face recognition, and show that this loss function can perform much better than the existing functions for deep learning in this domain.
\item We demonstrate how such models can then be used to perform joins across different surface forms of the same entity efficiently, with a complexity of $\mathcal{O}(n\log{}n)$.  
\end{itemize} 

The rest of the paper is organized as follows: Section~\ref{siamese networks} describes how we adapted siamese networks to the problem of entity matching, specifically the problem of triplet selection for entity names, and the need for a new type of loss function to improve metric learning for this problem, section ~\ref{joins} describes how to use these models for efficient joins across datasets, section ~\ref{dataset} describes characteristics of the datasets we used for training, as well as cleansing and data augmentation techniques we used to generate data, section ~\ref{results} provides the experimental results, section~\ref{related work} covers related work, and section~\ref{conclusions} describes future work and conclusions.
